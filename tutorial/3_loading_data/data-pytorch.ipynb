{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data with PyTorch\n",
    "In this notebook we will investigate a few different ways to handle data with PyTorch on Alvis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using your own data\n",
    "In many cases you have a dataset in mind that you've already acquired and are keeping in your home folder or perhaps more probable in a storage project.\n",
    "\n",
    "When it comes to using datasets in training datasets the most efficient approach that we have found to work on Alvis is to use utilities to directly stream data from uncompressed tar-archives or zip-archives (though highly compressed zip files can also sometimes be slow).\n",
    "\n",
    "In this section we will use the tiny-ImageNet dataset in `/mimer/NOBACKUP/Datasets` but with the hope that you can adapt it to any dataset that you have in your project storage. First let us take a look at the dataset.\n",
    "\n",
    "### Investigating the contents\n",
    "Let's take a look at what is contained in this archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "# Look at the structure of the zipfile\n",
    "path_to_dataset = '/mimer/NOBACKUP/Datasets/tiny-imagenet-200/tiny-imagenet-200.zip'\n",
    "with ZipFile(path_to_dataset, 'r') as datazip:\n",
    "    print(f\"Number of entries in the zipfile {len(datazip.namelist())}\")\n",
    "    print(*datazip.namelist()[:7], \"...\", *datazip.namelist()[-3:], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE:*** Investigating files like this can be quite slow if the archives are very large. Looking at the first few files are fast and can be good to get a sense of the file, but you don't want to have to search through them every time. If there is a README in connection with the dataset it is wise to take a look at it. Furthermore, you might want to note down the structure inside the archive yourself if it isn't in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at one of the txt files next\n",
    "with ZipFile(path_to_dataset, \"r\") as datazip:\n",
    "    print(datazip.read(\"tiny-imagenet-200/wnids.txt\").decode(\"utf8\").split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will later be used as the labels for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the number of train files like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(path_to_dataset) as datazip:\n",
    "    print(len([fn for fn in datazip.namelist() if 'train' in fn and fn.endswith('.JPEG')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading file information from zip-files is fast as they have information about all of its members easily retriveable. For a tarfile you would have to traverse the entire archive (with e.g. `tarfile.TarFile.getmembers`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Visualize images\n",
    "fig, ax_grid = plt.subplots(3, 3, figsize=(15, 15))\n",
    "with ZipFile(path_to_dataset) as datazip:\n",
    "    # Get filenames of training images\n",
    "    filenames = [fn for fn in datazip.namelist() if 'train' in fn and fn.endswith('.JPEG')]\n",
    "    for ax, fn in zip(ax_grid.flatten(), filenames):\n",
    "        # Get path to file and label\n",
    "        label = fn.split(\"/\")[-1].split('_')[0]\n",
    "    \n",
    "        # Add to axis\n",
    "        img = plt.imread(BytesIO(datazip.read(fn)), format=\"jpg\")\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'Label {label}')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be worth noting that the image labels are listed in wnids.txt that can be found in the archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier from this data\n",
    "Now we have some understanding of what the database does and we are ready to do some ML on it.\n",
    "\n",
    "First we will define our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader",
    "\n",
    "# For performance set precision,\n",
    "# see https://www.c3se.chalmers.se/documentation/applications/pytorch/#performance-and-precision\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use torch.hub to load a pretrained model\n",
    "efficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "#preprocessing_utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n",
    "\n",
    "# We freeze all parameters except the last layer\n",
    "freeze_blocks = [efficientnet.stem] + [\n",
    "    layer\n",
    "    for layer in efficientnet.layers\n",
    "    if layer != efficientnet.classifier\n",
    "]\n",
    "for block in freeze_blocks:\n",
    "    for parameter in block:\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "# Modify the number of output classes to 200\n",
    "efficientnet.classifier.fc = nn.Linear(efficientnet.classifier.fc.in_features, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efficientnet\n",
    "opt = optim.Adam(model.parameters(), lr=0.003)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will construct the dataloader from a datapipe. Compared to previous datapipes we will also add:\n",
    " - possibility to shuffle data\n",
    " - at the end construct batchable tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Construct a Dataset class for our dataset\n",
    "class TinyImageNetDataset(Dataset):\n",
    "    def __init__(self, path_to_dataset: str, split: str):\n",
    "        if split not in [\"train\", \"val\", \"test\"]:\n",
    "            raise ValueError(\"Invalid split, select 'train', 'val' or 'test'.\")\n",
    "        if split in [\"val\", \"test\"]:\n",
    "            raise NotImplementedError(\"Only train split is currently implemented.\")\n",
    "        \n",
    "        self.zfpath = path_to_dataset\n",
    "\n",
    "        # Avoid reusing the file handle created here, for known issue with multi-worker:\n",
    "        # https://discuss.pytorch.org/t/dataloader-with-zipfile-failed/42795\n",
    "        self.zf = None\n",
    "        with ZipFile(self.zfpath) as zf:\n",
    "            # Get images from split\n",
    "            self.imglist: list[str] = [\n",
    "                path for path in zf.namelist()\n",
    "                if split in path\n",
    "                and path.endswith(\".JPEG\")\n",
    "            ]\n",
    "\n",
    "            # Get look-up dictionary for word name ID to label\n",
    "            wnids = zf.read(\"tiny-imagenet-200/wnids.txt\").decode(\"utf8\").split()\n",
    "            self.wnid2label: dict[str, int] = {wnid: label for label, wnid in enumerate(wnids)}\n",
    "\n",
    "    def get_label(self, path: str) -> int:\n",
    "        if not path.endswith(\".JPEG\"):\n",
    "            raise ValueError(f\"Expected path to image, got {path}\")\n",
    "        word_name_id: str = fn.split(\"/\")[-1].split('_')[0]\n",
    "        return self.wnid2label[word_name_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imglist)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[Image.Image, int]:\n",
    "        if self.zf is None:\n",
    "            self.zf=ZipFile(self.zfpath)\n",
    "\n",
    "        # Convert image to Tensor of size (Channel, Px, Py)\n",
    "        imgpath = self.imglist[idx]\n",
    "        img_array = np.array(Image.open(BytesIO(self.zf.read(imgpath))))\n",
    "        if img_array.ndim < 3:\n",
    "            # Greyscale to RGB\n",
    "            img_array = np.repeat(img_array[..., np.newaxis], 3, -1)\n",
    "\n",
    "        img_tensor = torch.from_numpy(img_array)\n",
    "        img_tensor = img_tensor.permute(2,0,1)\n",
    "                       \n",
    "        # Get label from filename\n",
    "        label = self.get_label(imgpath)\n",
    "        return img_tensor.float(), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataloader from dataset\n",
    "dataset = TinyImageNetDataset(path_to_dataset, split=\"train\")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(\n",
    "    dataloader,\n",
    "    model,\n",
    "    opt,\n",
    "    loss_func,\n",
    "    n_epochs=3,\n",
    "    device=torch.device(\"cuda:0\"),\n",
    "):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        loss_sum = 0.0\n",
    "        n_correct = 0\n",
    "        for i_batch, (x, label) in enumerate(dataloader):\n",
    "            print('\\r' + f'Batch: {i_batch}/{len(dataloader)}', end='')\n",
    "            x, label = x.to(device), label.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = loss_func(logits, label)\n",
    "            \n",
    "            loss_sum += loss.item()\n",
    "            n_correct += (logits.argmax(1) == label).long().sum()\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        avg_loss = loss_sum / (i_batch + 1)\n",
    "        accuracy = n_correct / len(dataloader.dataset)\n",
    "        print(f\" Loss: {avg_loss}\", f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train(dataloader, model, opt, loss_func, n_epochs=3);\n",
    "# If you get an error message \"Unable to find a valid cuDNN algorithm\"\n",
    "# then, you're probably running out of GPU memory and should kill other\n",
    "# processes using up memory and/or reduce the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tasks\n",
    " 1. Make yourself acquainted with the above code.\n",
    " 2. Take a look at `jobscript-pytorch.sh` to see how you would go about training something non-interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using available datasets\n",
    "Some common public datasets are available at `/mimer/NOBACKUP/Datasets`, if there are some specific dataset you would like to see added you can create a request through [support](https://supr.naiss.se/support/).\n",
    "\n",
    "In this part we will access the processed MNIST dataset available at `/mimer/NOBACKUP/Datasets/MNIST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 (3, 3) convolutional filters followed by a dense layer\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 10, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(6760, 10),\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case it is really simple as this dataset has been processed for use with `torchvision.datasets.MNIST` and all we need to do is supply the correct path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "dataset = datasets.MNIST(\"/mimer/NOBACKUP/Datasets\", transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dataloader, model, opt, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data through the torchvision API\n",
    "At `torchvision.datasets`, `torchaudio.datasets` and `torchtext.datasets` all have similar APIs that can be used to download datasets that do not exist in `/mimer/NOBACKUP/Datasets`. However, note that this can take some time and you will have to store them yourself. If you are interested in a dataset that permit us redistributing it, then contact us through the regular support form and we can look at storing it centrally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
